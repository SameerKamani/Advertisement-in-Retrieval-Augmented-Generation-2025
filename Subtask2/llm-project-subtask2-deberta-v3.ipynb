{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11116088,"sourceType":"datasetVersion","datasetId":6931083}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install necessary libraries if not already available\n!pip install datasets transformers evaluate --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:19:39.117010Z","iopub.execute_input":"2025-05-25T14:19:39.117599Z","iopub.status.idle":"2025-05-25T14:19:44.128738Z","shell.execute_reply.started":"2025-05-25T14:19:39.117571Z","shell.execute_reply":"2025-05-25T14:19:44.128051Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Import libraries, configure warnings, and set up configurations\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Suppress specific warning from torch\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Was asked to gather along dimension 0\")\n\n# Datasets for handling data, Evaluate for metrics\nfrom datasets import Dataset, DatasetDict\nimport evaluate\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:19:44.129624Z","iopub.execute_input":"2025-05-25T14:19:44.129811Z","iopub.status.idle":"2025-05-25T14:20:13.151211Z","shell.execute_reply.started":"2025-05-25T14:19:44.129790Z","shell.execute_reply":"2025-05-25T14:20:13.150622Z"}},"outputs":[{"name":"stderr","text":"2025-05-25 14:19:55.642017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748182795.834154      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748182795.887727      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Data setup\nDATA_PATH = \"/kaggle/input/touche-task4-2025-subtask2\"\ntrain_responses_file = os.path.join(DATA_PATH, \"responses-train.jsonl\")\ntrain_labels_file = os.path.join(DATA_PATH, \"responses-train-labels.jsonl\")\nval_responses_file = os.path.join(DATA_PATH, \"responses-validation.jsonl\")\nval_labels_file = os.path.join(DATA_PATH, \"responses-validation-labels.jsonl\")\ntest_responses_file = os.path.join(DATA_PATH, \"responses-test.jsonl\")\ntest_labels_file = os.path.join(DATA_PATH, \"responses-test-labels.jsonl\")\n\ndef load_jsonl(file_path):\n    \"\"\"Load a JSONL file and return a list of dicts.\"\"\"\n    data = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                data.append(json.loads(line))\n    return data\n\ndef merge_data(responses_file, labels_file):\n    \"\"\"Merge responses and labels using the response id.\"\"\"\n    responses = load_jsonl(responses_file)\n    labels = load_jsonl(labels_file)\n    label_map = {item[\"id\"]: item[\"label\"] for item in labels}\n    \n    merged = []\n    for resp in responses:\n        rid = resp[\"id\"]\n        if rid in label_map:\n            merged.append({\n                \"id\": rid,\n                \"text\": resp[\"response\"],\n                \"label\": label_map[rid]\n            })\n    return merged","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:21:55.536796Z","iopub.execute_input":"2025-05-25T14:21:55.537139Z","iopub.status.idle":"2025-05-25T14:21:55.543690Z","shell.execute_reply.started":"2025-05-25T14:21:55.537111Z","shell.execute_reply":"2025-05-25T14:21:55.543013Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 4: Create Hugging Face Datasets for train/validation/test\ntrain_data = merge_data(train_responses_file, train_labels_file)\nval_data = merge_data(val_responses_file, val_labels_file)\ntest_data = merge_data(test_responses_file, test_labels_file)\n\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\ntest_dataset = Dataset.from_list(test_data)\n\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset\n})\n\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:21:58.292048Z","iopub.execute_input":"2025-05-25T14:21:58.292756Z","iopub.status.idle":"2025-05-25T14:21:59.058216Z","shell.execute_reply.started":"2025-05-25T14:21:58.292735Z","shell.execute_reply":"2025-05-25T14:21:59.057407Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'label'],\n        num_rows: 11487\n    })\n    validation: Dataset({\n        features: ['id', 'text', 'label'],\n        num_rows: 3257\n    })\n    test: Dataset({\n        features: ['id', 'text', 'label'],\n        num_rows: 2600\n    })\n})\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 5: Tokenize using DeBERTa V3 Base tokenizer\n\nmodel_checkpoint = \"microsoft/deberta-v3-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ndef tokenize_function(examples):\n    # Reduce the maximum length to 256 tokens to save memory\n    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"id\", \"text\"])\n\n# Suppress the sentencepiece warning\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:22:04.690151Z","iopub.execute_input":"2025-05-25T14:22:04.690958Z","iopub.status.idle":"2025-05-25T14:22:16.014609Z","shell.execute_reply.started":"2025-05-25T14:22:04.690934Z","shell.execute_reply":"2025-05-25T14:22:16.013994Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9012962fa4b940fe86c11dc1b44a773e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"041574e1a10f4155b48b69d2714c4cce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97620b7d96724d2a92577fc485f63f78"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11487 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e54ec6370274b54b7afa10b95cd7077"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3257 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb9f10e786a41a99e0cb3f37c03eac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d1db080cda044b3b85898c9b0205b46"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Cell 6: Create the model and data collator\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:20:13.294856Z","iopub.status.idle":"2025-05-25T14:20:13.295145Z","shell.execute_reply.started":"2025-05-25T14:20:13.294985Z","shell.execute_reply":"2025-05-25T14:20:13.294999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: Define evaluation metrics and configure the Trainer\n\nimport evaluate\n\n# Load evaluation metrics\naccuracy_metric = evaluate.load(\"accuracy\")\nprecision_metric = evaluate.load(\"precision\")\nrecall_metric = evaluate.load(\"recall\")\nf1_metric = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"binary\")[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"binary\")[\"recall\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"binary\")[\"f1\"]\n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# Enable gradient checkpointing to reduce memory consumption during training\nmodel.gradient_checkpointing_enable()\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,       # Further reduced batch size to lower memory usage\n    per_device_eval_batch_size=8,        # Reduced evaluation batch size as well\n    gradient_accumulation_steps=8,       # Accumulate gradients to simulate an effective batch size of 4\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_steps=50,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    report_to=\"none\",\n    fp16=True                          # Enable mixed precision training for memory efficiency\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:20:13.296040Z","iopub.status.idle":"2025-05-25T14:20:13.296373Z","shell.execute_reply.started":"2025-05-25T14:20:13.296216Z","shell.execute_reply":"2025-05-25T14:20:13.296231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Train the model\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:20:13.297628Z","iopub.status.idle":"2025-05-25T14:20:13.297951Z","shell.execute_reply.started":"2025-05-25T14:20:13.297793Z","shell.execute_reply":"2025-05-25T14:20:13.297808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Evaluate on the test set\ntest_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\nprint(\"Test set metrics:\", test_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:20:13.298776Z","iopub.status.idle":"2025-05-25T14:20:13.299056Z","shell.execute_reply.started":"2025-05-25T14:20:13.298891Z","shell.execute_reply":"2025-05-25T14:20:13.298907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Save the fine-tuned model and tokenizer\nmodel_save_path = \"./deberta-v3-large-ad-detector-finetuned\"\ntrainer.save_model(model_save_path)\ntokenizer.save_pretrained(model_save_path)\nprint(f\"Model saved to {model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:20:13.300278Z","iopub.status.idle":"2025-05-25T14:20:13.300486Z","shell.execute_reply.started":"2025-05-25T14:20:13.300387Z","shell.execute_reply":"2025-05-25T14:20:13.300396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11: Compute and plot the confusion matrix for the test set\ntest_output = trainer.predict(tokenized_datasets[\"test\"])\npredictions = test_output.predictions.argmax(axis=-1)\ntrue_labels = test_output.label_ids\n\ncm = confusion_matrix(true_labels, predictions)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix on Test Set\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:20:13.301254Z","iopub.status.idle":"2025-05-25T14:20:13.301456Z","shell.execute_reply.started":"2025-05-25T14:20:13.301359Z","shell.execute_reply":"2025-05-25T14:20:13.301368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: Extract training loss logs and plot average loss per epoch\nlog_history = trainer.state.log_history\nepoch_loss = {}\nfor entry in log_history:\n    if \"epoch\" in entry and \"loss\" in entry:\n        epoch = entry[\"epoch\"]\n        if epoch not in epoch_loss:\n            epoch_loss[epoch] = []\n        epoch_loss[epoch].append(entry[\"loss\"])\n\nepoch_loss_avg = {epoch: sum(losses) / len(losses) for epoch, losses in epoch_loss.items()}\nsorted_epochs = sorted(epoch_loss_avg.keys())\nsorted_loss = [epoch_loss_avg[epoch] for epoch in sorted_epochs]\n\nplt.figure(figsize=(8,5))\nplt.plot(sorted_epochs, sorted_loss, marker=\"o\", linestyle=\"-\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Average Loss\")\nplt.title(\"Average Training Loss vs. Epoch\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:20:13.303487Z","iopub.status.idle":"2025-05-25T14:20:13.303722Z","shell.execute_reply.started":"2025-05-25T14:20:13.303613Z","shell.execute_reply":"2025-05-25T14:20:13.303623Z"}},"outputs":[],"execution_count":null}]}